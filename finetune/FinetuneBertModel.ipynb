{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f996065-781a-4bb1-ad46-b6105b34c647",
   "metadata": {},
   "source": [
    "Finetune BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292079c1-722f-4d1c-98e2-8ba353fb59d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ukumaras/bin/miniconda3/envs/mldb/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fc486-d03e-4c28-9c6a-a5dd697dbba2",
   "metadata": {},
   "source": [
    "### Load iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf8b441-e7a5-4ce3-b89d-c609d47c4c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  stage\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0  train\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0  train\n",
       "2  If only to avoid making this type of film in t...      0  train\n",
       "3  This film was probably inspired by Godard's Ma...      0  train\n",
       "4  Oh, brother...after hearing about this ridicul...      0  train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(\"../imdb.db\")\n",
    "imdb = con.sql(\"SELECT * FROM imdb\").df()\n",
    "con.close()\n",
    "\n",
    "imdb['label'] = imdb['label'].astype(int)\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4299ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test = Dataset.from_pandas(imdb[imdb['stage'] == 'test'].drop(columns=['stage']))\n",
    "imdb_train = Dataset.from_pandas(imdb[imdb['stage'] == 'train'].drop(columns=['stage']))\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = imdb_train\n",
    "dataset['test'] = imdb_test\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae7aa0-cd89-496e-a570-06593cf74d72",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ae6c61-df7d-48f8-b35d-31bb2b2a65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-tiny'\n",
    "model_path = f\"/homes/ukumaras/scratch/Models/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bert_model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510c57e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:34<00:00, 723.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:20<00:00, 1213.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    return tokenizer(batch['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset_encoded = dataset.map(preprocess, batched=True, batch_size=None)\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "# tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cbe885-6c0f-4d22-91e6-44f6cbba1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4469ab5-032a-4b7d-aa88-9ad4f262b381",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44ea04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /homes/ukumaras/scratch/Models/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_path, num_labels=num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75714f4-8691-41de-8858-d5464890204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8408efe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ukumaras/bin/miniconda3/envs/mldb/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(dataset_encoded[\"train\"])\n",
    "model_name = f\"{model_name}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False, \n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2195763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"test\"],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4abb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1955' max='1955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1955/1955 2:09:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531839</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>0.752580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.422355</td>\n",
       "      <td>0.815720</td>\n",
       "      <td>0.815278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.381090</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371245</td>\n",
       "      <td>0.841200</td>\n",
       "      <td>0.840959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363759</td>\n",
       "      <td>0.846040</td>\n",
       "      <td>0.845949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1955, training_loss=0.44344279491687977, metrics={'train_runtime': 7775.6458, 'train_samples_per_second': 16.076, 'train_steps_per_second': 0.251, 'total_flos': 158810880000000.0, 'train_loss': 0.44344279491687977, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a021aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path + '-imdb-cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df0ec23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b5b547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ukumaras/bin/miniconda3/envs/mldb/lib/python3.12/site-packages/torch/onnx/utils.py:1547: OnnxExporterWarning: Exporting to ONNX opset version 19 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 17. To use a newer opset version, consider 'torch.onnx.dynamo_export()'. Note that dynamo_export() is in preview. Please report errors with dynamo_export() as Github issues to https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple(dummy_model_input.values()),\n",
    "    f=model_path+\"/model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=19, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9611bc5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfail\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fail' is not defined"
     ]
    }
   ],
   "source": [
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2a26c-ee37-4b1b-ae61-4aa481276e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"/homes/ukumaras/scratch/Models/distilbert-base-uncased-finetuned-sst-2-english-onnx\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_model_path)\n",
    "session = InferenceSession(onnx_model_path + \"/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = session.run(output_names=[\"logits\"], input_feed=dict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef764400",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e45952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n",
    "\n",
    "config = DistilBertConfig()\n",
    "onnx_config = DistilBertOnnxConfig(config)\n",
    "print(list(onnx_config.outputs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832110",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020e86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
