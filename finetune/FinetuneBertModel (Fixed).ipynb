{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f996065-781a-4bb1-ad46-b6105b34c647",
   "metadata": {},
   "source": [
    "Finetune BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292079c1-722f-4d1c-98e2-8ba353fb59d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ukumaras/scratch/miniconda3/envs/mldb/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fc486-d03e-4c28-9c6a-a5dd697dbba2",
   "metadata": {},
   "source": [
    "### Load iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf8b441-e7a5-4ce3-b89d-c609d47c4c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  stage\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0  train\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0  train\n",
       "2  If only to avoid making this type of film in t...      0  train\n",
       "3  This film was probably inspired by Godard's Ma...      0  train\n",
       "4  Oh, brother...after hearing about this ridicul...      0  train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(\"../imdb.db\")\n",
    "imdb = con.sql(\"SELECT * FROM imdb\").df()\n",
    "con.close()\n",
    "\n",
    "imdb['label'] = imdb['label'].astype(int)\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4299ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test = Dataset.from_pandas(imdb[imdb['stage'] == 'test'].drop(columns=['stage']))\n",
    "imdb_train = Dataset.from_pandas(imdb[imdb['stage'] == 'train'].drop(columns=['stage']))\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = imdb_train\n",
    "dataset['test'] = imdb_test\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae7aa0-cd89-496e-a570-06593cf74d72",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ae6c61-df7d-48f8-b35d-31bb2b2a65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-tiny'\n",
    "model_path = f\"/homes/ukumaras/scratch/Models/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bert_model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510c57e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:11<00:00, 2142.38 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:11<00:00, 2136.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    return tokenizer(batch['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset_encoded = dataset.map(preprocess, batched=True, batch_size=None)\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "# tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cbe885-6c0f-4d22-91e6-44f6cbba1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea15c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_encoded['train']['token_type_ids'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4469ab5-032a-4b7d-aa88-9ad4f262b381",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44ea04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /homes/ukumaras/scratch/Models/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model_path, num_labels):\n",
    "        super(WrappedModel, self).__init__()\n",
    "        self.auto_model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                labels=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None):\n",
    "        return self.auto_model(input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels,\n",
    "                                position_ids=position_ids,\n",
    "                                head_mask=head_mask,\n",
    "                                inputs_embeds=inputs_embeds)\n",
    "model = WrappedModel(model_path, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75714f4-8691-41de-8858-d5464890204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8408efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(dataset_encoded[\"train\"])\n",
    "model_name = f\"{model_name}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False, \n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2195763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ukumaras/scratch/miniconda3/envs/mldb/lib/python3.12/site-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"test\"],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e4abb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1955' max='1955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1955/1955 40:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.536486</td>\n",
       "      <td>0.754480</td>\n",
       "      <td>0.754226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419991</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.816461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374859</td>\n",
       "      <td>0.836720</td>\n",
       "      <td>0.836717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.361707</td>\n",
       "      <td>0.844320</td>\n",
       "      <td>0.844263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.357170</td>\n",
       "      <td>0.847520</td>\n",
       "      <td>0.847517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1955, training_loss=0.44515237247242645, metrics={'train_runtime': 2417.8273, 'train_samples_per_second': 51.699, 'train_steps_per_second': 0.809, 'total_flos': 0.0, 'train_loss': 0.44515237247242645, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a021aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path + '-imdb-cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0ec23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22525f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 7099,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32107a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/homes/ukumaras/scratch/Models/bert-tiny'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b5b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple([dummy_model_input['input_ids'], dummy_model_input['attention_mask']]),\n",
    "    f=model_path+\"/model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=17, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fec2a26c-ee37-4b1b-ae61-4aa481276e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = f\"{model_path}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_model_path)\n",
    "session = InferenceSession(onnx_model_path + \"/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5723734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  2478,  4487, 16643, 23373,  2007,  2006, 26807,  2448,\n",
       "         7292,   999,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n",
    "del inputs['token_type_ids']\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd98a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = session.run(output_names=[\"logits\"], input_feed=dict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef764400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00219239, -0.47350207]], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e45952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n",
    "\n",
    "config = DistilBertConfig()\n",
    "onnx_config = DistilBertOnnxConfig(config)\n",
    "print(list(onnx_config.outputs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832110",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020e86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
